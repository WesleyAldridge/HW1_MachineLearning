{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1_MachineLearning_Problem1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WesleyAldridge/HW1_MachineLearning/blob/master/HW1_MachineLearning_Problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7ci243Btupkd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Problem 1:\n",
        "\n",
        "Use logistic regression with mean squared error loss.\n",
        "\n",
        "Instructions for problems 1 and 2:\n",
        "\n",
        "- Load the training and test data using Keras, no validation set needed.\n",
        "- Train 10 classifiers that perform binary classification: Is the input image the digit i or is it a digit different from i? Each of the ten classifiers has an input layer consisting of 28 x 28 input neurons and an output layer consisting of a single output neuron.\n",
        "- Implement mini-batch stochastic gradient descent using only numpy, that is, you are not allowed to use TensorFlow/Keras for SGD.\n",
        "- Use argmax to determine the classifier with the strongest output and declare the corresponding digit as output."
      ]
    },
    {
      "metadata": {
        "id": "AFUMeu-WjHkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O8g7kL8ujh4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid_double(x):\n",
        "    # Simple implementation of the sigmoid function for double values.\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_prime_double(x):\n",
        "    # Simple implementation of the derivative of of the sigmoid function for double values.\n",
        "    return sigmoid_double(x) * (1 - sigmoid_double(x))\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    # Simple implementation of the sigmoid function for vectors.\n",
        "    return np.vectorize(sigmoid_double)(z)\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # Simple implementation of the derivative of the sigmoid function for vectors.\n",
        "    return np.vectorize(sigmoid_prime_double)(z)\n",
        "\n",
        "\n",
        "class Layer:\n",
        "    # Layers are stacked to build a sequential neural network.\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "\n",
        "        # A layer know its predecessor (previous layer)\n",
        "        self.previous = None\n",
        "        # and its successor (next layer).\n",
        "        self.next = None\n",
        "\n",
        "        # Each layer can persist data flowing into and out of it\n",
        "        # in the forward pass.\n",
        "        self.input_data = None\n",
        "        self.output_data = None\n",
        "\n",
        "        # Analogously, a layer holds input and output data\n",
        "        # for the backward pass.\n",
        "        self.input_delta = None\n",
        "        self.output_delta = None\n",
        "\n",
        "    def connect(self, layer):\n",
        "        # This method connects a layer to its direct neighbors\n",
        "        # in the sequential network.\n",
        "        self.previous = layer\n",
        "        layer.next = self\n",
        "\n",
        "    def forward(self):\n",
        "        # Each layer implementation has to provide a function\n",
        "        # to feed input data forward.\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_forward_input(self):\n",
        "        # input_data is reserved for the first layer;\n",
        "        # all others get their inputs from the predecessors.\n",
        "        if self.previous is not None:\n",
        "            return self.previous.output_data\n",
        "        else:\n",
        "            return self.input_data\n",
        "\n",
        "    def backward(self):\n",
        "        # Layers have to implement backpropagation of error terms -\n",
        "        # a way to feed input errors backward through the network.\n",
        "        raise NotImplemented\n",
        "\n",
        "    def get_backward_input(self):\n",
        "        # input_delta is reserved for the last layer;\n",
        "        # all other layers get their error terms from their successors.\n",
        "        if self.next is not None:\n",
        "            return self.next.output_delta\n",
        "        else:\n",
        "            return self.input_delta\n",
        "\n",
        "    def clear_deltas(self):\n",
        "        # You compute and accumulate deltas per mini-batch,\n",
        "        # after which you need to reset these deltas.\n",
        "        pass\n",
        "\n",
        "    def update_params(self, learning_rate):\n",
        "        # Update layer parameters according to current deltas,\n",
        "        # using the specified learning_rate.\n",
        "        pass\n",
        "\n",
        "    def describe(self):\n",
        "        # Layer implementation can print their properties.\n",
        "        raise NotImplemented\n",
        "\n",
        "\n",
        "class ActivationLayer(Layer):\n",
        "    # This activation layer uses the sigmoid function\n",
        "    # to activate neurons.\n",
        "    def __init__(self, input_dim):\n",
        "        super(ActivationLayer, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = input_dim\n",
        "\n",
        "    def forward(self):\n",
        "        # The forward pass is simply applying\n",
        "        # the sigmoid function to the input_data.\n",
        "        data = self.get_forward_input()\n",
        "        self.output_data = sigmoid(data)\n",
        "        #print(\"activation layer\")\n",
        "        #print(\"  data:\", data.shape)\n",
        "        #print(\"  output_data:\", self.output_data.shape)\n",
        "\n",
        "    def backward(self):\n",
        "        # The backward pass is element-wise multiplication of\n",
        "        # the error term with the sigmoid derivative evaluated at\n",
        "        # the input to this layer.\n",
        "        delta = self.get_backward_input()\n",
        "        data = self.get_forward_input()\n",
        "        self.output_delta = delta * sigmoid_prime(data)\n",
        "\n",
        "    def describe(self):\n",
        "        print(\"|-- \" + self.__class__.__name__)\n",
        "        print(\"  |-- dimensions: ({} {})\".format(self.input_dim, self.output_dim))\n",
        "\n",
        "\n",
        "class DenseLayer(Layer):\n",
        "    # Dense layers have input and output dimensions.\n",
        "    def __init__(self,  input_dim, output_dim):\n",
        "\n",
        "        super(DenseLayer, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Randomly initialize weight matrix and bias vector.\n",
        "        self.weight = np.random.randn(output_dim, input_dim)\n",
        "        self.bias = np.random.randn(output_dim, 1)\n",
        "\n",
        "        # The layer parameters consist of weights and bias terms.\n",
        "        self.params = [self.weight, self.bias]\n",
        "\n",
        "        # Deltas for weights and biases are set to 0.\n",
        "        self.delta_w = np.zeros(self.weight.shape)\n",
        "        self.delta_b = np.zeros(self.bias.shape)\n",
        "\n",
        "    def forward(self):\n",
        "        # The forward pass of the dense layer is\n",
        "        # the affine-linear transformation on the input data\n",
        "        # described by weights and biases.\n",
        "        data = self.get_forward_input()\n",
        "        self.output_data = np.dot(self.weight, data) + self.bias\n",
        "        #print(\"dense layer\")\n",
        "        #print(\"  data:\", data.shape)\n",
        "        #print(\"  weight:\", self.weight.shape)\n",
        "        #print(\"  bias:\", self.bias.shape)\n",
        "        #print(\"  output_data:\", self.output_data.shape)\n",
        "\n",
        "    def backward(self):\n",
        "        # For the backward pass, you first get input data and delta.\n",
        "        data = self.get_forward_input()\n",
        "        delta = self.get_backward_input()\n",
        "\n",
        "        # The current delta is added to the bias delta.\n",
        "        self.delta_b += delta\n",
        "\n",
        "        # Then you add this term to the weight delta.\n",
        "        self.delta_w += np.dot(delta, data.transpose())\n",
        "\n",
        "        # The backward pass is completed by passing\n",
        "        # an output delta to the previous layer.\n",
        "        self.output_delta = np.dot(self.weight.transpose(), delta)\n",
        "\n",
        "    def update_params(self, rate):\n",
        "        # Using weight and bias deltas,\n",
        "        # you can update model parameters\n",
        "        # with gradient descent.\n",
        "        self.weight -= rate * self.delta_w\n",
        "        self.bias -= rate * self.delta_b\n",
        "\n",
        "    def clear_deltas(self):\n",
        "        # After updating parameters,\n",
        "        # you should reset all deltas.\n",
        "        self.delta_w = np.zeros(self.delta_w.shape)\n",
        "        self.delta_b = np.zeros(self.delta_b.shape)\n",
        "\n",
        "    def describe(self):\n",
        "        print(\"|--- \" + self.__class__.__name__)\n",
        "        print(\"  |-- dimensions: ({}, {})\".format(self.input_dim, self.output_dim))\n",
        "\n",
        "\n",
        "class MSE:\n",
        "    # Mean squared error loss function\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_function(predictions, labels):\n",
        "        diff = predictions - labels\n",
        "        # By defining MSE as 0.5 times\n",
        "        # the square difference between\n",
        "        # predictions and labels ...\n",
        "        return 0.5 * sum(diff * diff)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_derivative(predictions, labels):\n",
        "        # ... the loss derivative is simply\n",
        "        # the difference between predictions and labels\n",
        "        #print(\"predictions:\", predictions)\n",
        "        #print(\"labels:\", labels)\n",
        "        return predictions - labels\n",
        "\n",
        "\n",
        "class SequentialNetwork:\n",
        "    # In a sequential neural network, you stack layers sequentially.\n",
        "    def __init__(self, loss=None):\n",
        "        print(\"Initialize Network...\")\n",
        "        self.layers = []\n",
        "        # If no loss is provided, MSE is used.\n",
        "        if loss is None:\n",
        "            self.loss = MSE()\n",
        "\n",
        "    def add(self, layer):\n",
        "        # Whenever you add a layer, you connect it\n",
        "        # to its predecessor and let it describe itself.\n",
        "        self.layers.append(layer)\n",
        "        layer.describe()\n",
        "        if len(self.layers) > 1:\n",
        "            self.layers[-1].connect(self.layers[-2])\n",
        "\n",
        "    def train(self, training_data, epochs, mini_batch_size, learning_rate, test_data=None):\n",
        "        n = len(training_data)\n",
        "        # To train you network, you pass over the data\n",
        "        # for as many times as there are epochs.\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k + mini_batch_size] for\n",
        "                k in range(0, n, mini_batch_size)\n",
        "            ]\n",
        "            # For each mini batch, you train your network.\n",
        "            for mini_batch in mini_batches:\n",
        "                self.train_batch(mini_batch, learning_rate)\n",
        "            if test_data:\n",
        "                # If you provided test data,\n",
        "                # you evaluate your network on it after each epoch.\n",
        "                n_test = len(test_data)\n",
        "                print(\"Epoch {0}: {1} / {2}\".format(epoch, self.evaluate(test_data), n_test))\n",
        "            else:\n",
        "                print(\"Epoch {0} complete\".format(epoch))\n",
        "\n",
        "    def train_batch(self, mini_batch, learning_rate):\n",
        "        # To train the network you,\n",
        "        # compute feed-forward and backward pass...\n",
        "        self.forward_backward(mini_batch)\n",
        "        # ... and then update model parameters accordingly.\n",
        "        self.update(mini_batch, learning_rate)\n",
        "\n",
        "    def update(self, mini_batch, learning_rate):\n",
        "        # A common technique is to normalize\n",
        "        # the learning rate by the mini-batch size.\n",
        "        learning_rate = learning_rate / len(mini_batch)\n",
        "\n",
        "        # Update all layers\n",
        "        for layer in self.layers:\n",
        "            layer.update_params(learning_rate)\n",
        "        # Clear all deltas in each layer.\n",
        "        for layer in self.layers:\n",
        "            layer.clear_deltas()\n",
        "\n",
        "    def forward_backward(self, mini_batch):\n",
        "        for x, y in mini_batch:\n",
        "            self.layers[0].input_data = x\n",
        "            # For each sample in the mini batch,\n",
        "            # feed the features forward layer by layer.\n",
        "            for layer in self.layers:\n",
        "                layer.forward()\n",
        "            # Compute the loss derivative for the output data.\n",
        "            self.layers[-1].input_delta = \\\n",
        "                self.loss.loss_derivative(self.layers[-1].output_data, y)\n",
        "            # Do layer-by-layer backpropagation of error terms.\n",
        "            for layer in reversed(self.layers):\n",
        "                layer.backward()\n",
        "\n",
        "    def single_forward(self, x):\n",
        "        # Pass a single sample forward and\n",
        "        # return result.\n",
        "        self.layers[0].input_data = x\n",
        "        for layer in self.layers:\n",
        "            layer.forward()\n",
        "        return self.layers[-1].output_data\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        # Compute accuracy on test data.\n",
        "        test_results = [(\n",
        "            np.argmax(self.single_forward(x)),\n",
        "            np.argmax(y)\n",
        "        ) for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E90oQllQ4ZCH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1:\n",
        "##Load the training and test data using Keras, no validation set needed"
      ]
    },
    {
      "metadata": {
        "id": "Q4umfgH9zb2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "910195c4-28d2-48eb-e9ef-e1a5ace5375d"
      },
      "cell_type": "code",
      "source": [
        "# load train and test data\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()\n",
        "\n",
        "print(train_images_original.shape)\n",
        "print(train_labels_original.shape)\n",
        "example_index = 1\n",
        "plt.figure()\n",
        "_ = plt.imshow(np.reshape(train_images_original[example_index, :], (28,28)), 'gray')\n",
        "print(train_labels_original[1])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAhLADAgLAHAgLAEAIP/ApmeJZaixpdjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "S_9dvNT78oie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "7323114d-d151-4731-e191-f024e0856194"
      },
      "cell_type": "code",
      "source": [
        "print(test_images_original.shape)\n",
        "print(test_labels_original.shape)\n",
        "\n",
        "example_index = 1\n",
        "plt.figure()\n",
        "_ = plt.imshow(np.reshape(test_images_original[example_index, :], (28,28)), 'gray')\n",
        "print(test_labels_original[1])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "khNvh0F54pBv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2: \n",
        "##Train 10 classifiers that perform binary classification: Is the input image the digit i or is it a digit different from i? Each of the ten classifiers has an input layer consisting of 28 x 28 input neurons and an output layer consisting of a single output neuron."
      ]
    },
    {
      "metadata": {
        "id": "8zzPrRio3OqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3ed8c5f9-1ac5-4759-a8bb-641a3357fa38"
      },
      "cell_type": "code",
      "source": [
        "# preprocess data\n",
        "\n",
        "# 60,000 images (?)\n",
        "# 28 * 28 = 784 input neurons per classifier\n",
        "# 1 output neuron per classifier\n",
        "\n",
        "train_images = train_images_original.reshape((60000, 28 * 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels_original)\n",
        "train_labels = train_labels.reshape((60000, 10, 1))\n",
        "\n",
        "test_labels = to_categorical(test_labels_original)\n",
        "test_labels = test_labels.reshape(10000, 10, 1)\n",
        "\n",
        "train_data = [(train_images[i], train_labels[i]) for i in range(0, 60000)]\n",
        "test_data = [(test_images[i], test_labels[i]) for i in range(0, 10000)]\n",
        "\n",
        "print(train_images.shape)\n",
        "print(train_labels_original)\n",
        "print(test_labels_original)\n",
        "print(train_labels[0])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784, 1)\n",
            "[5 0 4 ... 5 6 8]\n",
            "[7 2 1 ... 4 5 6]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y7VCIyBykHps",
        "colab_type": "code",
        "outputId": "c060fe6a-f28b-4faa-dbef-e04b1f59f3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#print(train_images.shape)\n",
        "#print(train_labels.shape)\n",
        "\n",
        "#print(test_images.shape)\n",
        "#print(test_labels.shape)\n",
        "\n",
        "net = SequentialNetwork()\n",
        "net.add(DenseLayer(784, 100))\n",
        "net.add(ActivationLayer(100))\n",
        "net.add(DenseLayer(100, 10))\n",
        "net.add(ActivationLayer(10))\n",
        "\n",
        "#net.add(DenseLayer(392, 196))\n",
        "#net.add(ActivationLayer(196))\n",
        "#net.add(DenseLayer(196, 10))\n",
        "#net.add(ActivationLayer(10))\n",
        "\n",
        "net.train(train_data,\n",
        "          epochs=10,\n",
        "          mini_batch_size=10,\n",
        "          learning_rate=3.0,\n",
        "          test_data=test_data)\n",
        "\n",
        "#print(net.single_forward(test_data[0][0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "Initialize Network...\n",
            "|--- DenseLayer\n",
            "  |-- dimensions: (784, 100)\n",
            "|-- ActivationLayer\n",
            "  |-- dimensions: (100 100)\n",
            "|--- DenseLayer\n",
            "  |-- dimensions: (100, 10)\n",
            "|-- ActivationLayer\n",
            "  |-- dimensions: (10 10)\n",
            "Epoch 0: 4959 / 10000\n",
            "Epoch 1: 6823 / 10000\n",
            "Epoch 2: 6866 / 10000\n",
            "Epoch 3: 6894 / 10000\n",
            "Epoch 4: 6925 / 10000\n",
            "Epoch 5: 6979 / 10000\n",
            "Epoch 6: 7000 / 10000\n",
            "Epoch 7: 6994 / 10000\n",
            "Epoch 8: 7061 / 10000\n",
            "Epoch 9: 7763 / 10000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}